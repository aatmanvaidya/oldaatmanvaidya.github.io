---
---

@string{aaai = {Association for the Advancement of Artificial Intelligence,}}
@string{acm = {Association for Computing Machinery,}}
@string{arxiv = {arXiv,}}
@string{acl = {Association for Computational Linguistics,}}

% all the fileds - abbr, title, preview (conf image), author, abstract, booktitle, year, publisher, pdf, metatype, html, url, doi, selected, poster, slides, award, dataset.
% see the _layouts/bib.html file for all the types 

@inproceedings{arora2023uli,
  abbr={NAACL 2024},
  title={The Uli Dataset: An Exercise in Experience Led Annotation of oGBV},
  author={Arora, Arnav and Jinadoss, Maha and Arora, Cheshta and George, Denny and Brindaalakshmi and .... and Vaidya, Aatman and Prabhakar, Tarunima},
  abstract={Online gender-based violence has grown concomitantly with the adoption of the internet and social media. Its effects are worse in the Global majority where many users use social media in languages other than English. The scale and volume of conversations on the internet have necessitated the need for automated detection of hate speech and, more specifically, gendered abuse. There is, however, a lack of language-specific and contextual data to build such automated tools. In this paper, we present a dataset on gendered abuse in three languages- Hindi, Tamil and Indian English. The dataset comprises of tweets annotated along three questions pertaining to the experience of gender abuse, by experts who identify as women or a member of the LGBTQIA+ community in South Asia. Through this dataset, we demonstrate a participatory approach to creating datasets that drive AI systems.},
  booktitle={The 8th Workshop on Online Abuse and Harms at Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2024},
  publisher={@acl},
  html={https://arxiv.org/abs/2311.09086},
  pdf={../papers/ogbv_dataset.pdf},
  award={Outstanding Paper Award},
  dataset={https://github.com/tattle-made/uli_dataset},
  metatype={published}
}

@inproceedings{vaidya2024analysing,
  abbr={CODS-COMAD},
  title={Analysing the Spread of Toxicity on Twitter},
  author={Vaidya, Aatman and Nagar, Seema and Nanavati, Amit A},
  abstract={The spread of hate speech on social media platforms has become a rising concern in recent years. Understanding the spread of hate is crucial for mitigating its harmful effects and fostering a healthier online environment. In this paper, we propose a new model to capture the evolution of toxicity in a network -- if a tweet with a certain toxicity (hatefulness) is posted, how much toxic a social network will become after a given number of rounds. We compute a toxicity score for each tweet, indicating the extent of the hatefulness of that tweet. Toxicity spread has not been adequately addressed in the existing literature. The two popular paradigms for modelling information spread, namely the Susceptible-Infected-Recovered (SIR) and its variants, as well as the spreading-activation models (SPA), are not suitable for modelling toxicity spread. The first paradigm employs a threshold and categorizes tweets as either toxic or non-toxic, while the second paradigm treats hate as energy and applies energy-conversion principles to model its propagation. Through analysis of a Twitter dataset consisting of $19.58$ million tweets, we observe that the total toxicity, as well as the average toxicity of original tweets and retweets in the network, does not remain constant but rather increases over time. In this paper, we propose a new method for toxicity spread. First, we categorize users into three distinct groups: Amplifiers, Attenuators, and Copycats. These categories are assigned based on the exchange of toxicity by a user, with Amplifiers sending out more toxicity than they receive, Attenuators experiencing a higher influx of toxicity compared to what they generate, and Copycats simply mirroring the hate they receive. We perform extensive experimentation on Barabási–Albert (BA) graphs, as well as subgraphs extracted from the Twitter dataset. Our model is able to replicate the patterns of toxicity.},
  booktitle={Proceedings of the 7th Joint International Conference on Data Science \& Management of Data (11th ACM IKDD CODS and 29th COMAD)},
  year={2024},
  pages={118--126},
  publisher={@acm},
  html={https://dl.acm.org/doi/10.1145/3632410.3632436},
  metatype={published}
}

@article{vaidya2024overview,
  abbr={ICON 2023},
  title={Overview of the 2023 ICON Shared Task on Gendered Abuse Detection in Indic Languages},
  author={Vaidya, Aatman and Arora, Arnav and Joshi, Aditya and Prabhakar, Tarunima},
  abstract={This paper reports the findings of the ICON 2023 on Gendered Abuse Detection in Indic Languages. The shared task deals with the detection of gendered abuse in online text. The shared task was conducted as a part of ICON 2023, based on a novel dataset in Hindi, Tamil and the Indian dialect of English. The participants were given three subtasks with the train dataset consisting of approximately 6500 posts sourced from Twitter. For the test set, approximately 1200 posts were provided. The shared task received a total of 9 registrations. The best F-1 scores are 0.616 for subtask 1, 0.572 for subtask 2 and, 0.616 and 0.582 for subtask 3. The paper contains examples of hateful content owing to its topic.},
  journal={The 20th International Conference on Natural Language Processing},
  year={2023},
  html={https://arxiv.org/abs/2401.03677},
  metatype={published}
}
